/home/arne/miniconda3/envs/incplex/bin/python /home/arne/projects/dfki/sha-rnn/main.py --epochs 20 --max-steps-per-epoch 100 --dropouth 0.1 --dropouti 0.1 --dropout 0.1 --data data/enwik8/ --save ENWIK8.pt --log-interval 10 --seed 5512 --optimizer lamb --bptt 1024 --warmup 800 --lr 2e-3 --emsize 64 --nhid 64 --nlayers 4 --batch-size 32 --discard-highest-losses 0.1 --enlarge-model-every-n-epochs 5
Loading cached dataset...
Eval batch size of 32
Total number of tokens: 205
Using []
enlarge model: emsize=21, nhid=21 (discard_highest_losses=0.1)
Args: Namespace(accumulate=1, alpha=2, batch_size=32, beta=1, bptt=1024, clip=0.25, cooldown=None, cuda=True, data='data/enwik8/', discard_highest_losses=0.1, dropout=0.1, dropoute=0.1, dropouth=0.1, dropouti=0.1, emsize=64, enlarge_model_every_n_epochs=5, epochs=20, log_interval=10, lr=0.002, max_steps_per_epoch=100, model='LSTM', nhid=64, nlayers=4, nonmono=5, optimizer='lamb', resume='', save='ENWIK8.pt', seed=5512, tied=True, warmup=800, wdecay=1.2e-06, wdrop=0.0, when=[-1])
Model total parameters: 23585
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
| epoch   0 |    10/  100 batches | lr 0.00003 | ms/batch 180.17 | loss  5.85 | ppl   346.12 | bpc    8.435
| epoch   0 |    20/  100 batches | lr 0.00005 | ms/batch 158.76 | loss  5.31 | ppl   202.17 | bpc    7.659
| epoch   0 |    30/  100 batches | lr 0.00008 | ms/batch 151.98 | loss  5.30 | ppl   199.92 | bpc    7.643
| epoch   0 |    40/  100 batches | lr 0.00010 | ms/batch 151.95 | loss  5.28 | ppl   196.38 | bpc    7.618
| epoch   0 |    50/  100 batches | lr 0.00013 | ms/batch 152.04 | loss  5.26 | ppl   191.74 | bpc    7.583
| epoch   0 |    60/  100 batches | lr 0.00015 | ms/batch 145.78 | loss  5.23 | ppl   186.65 | bpc    7.544
| epoch   0 |    70/  100 batches | lr 0.00018 | ms/batch 158.37 | loss  5.20 | ppl   181.81 | bpc    7.506
| epoch   0 |    80/  100 batches | lr 0.00020 | ms/batch 154.23 | loss  5.17 | ppl   176.26 | bpc    7.462
| epoch   0 |    90/  100 batches | lr 0.00023 | ms/batch 152.87 | loss  5.14 | ppl   170.60 | bpc    7.414
| epoch   0 |   100/  100 batches | lr 0.00025 | ms/batch 151.44 | loss  5.10 | ppl   163.77 | bpc    7.356
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 23.99s | valid loss  5.06 | valid ppl   157.65 | valid bpc    7.301
-----------------------------------------------------------------------------------------
Saving model (new best validation)
| epoch   1 |    10/  100 batches | lr 0.00200 | ms/batch 163.59 | loss  5.39 | ppl   219.78 | bpc    7.780
| epoch   1 |    20/  100 batches | lr 0.00200 | ms/batch 154.79 | loss  4.57 | ppl    97.02 | bpc    6.600
| epoch   1 |    30/  100 batches | lr 0.00200 | ms/batch 147.05 | loss  4.42 | ppl    83.34 | bpc    6.381
| epoch   1 |    40/  100 batches | lr 0.00200 | ms/batch 154.12 | loss  4.38 | ppl    80.03 | bpc    6.322
| epoch   1 |    50/  100 batches | lr 0.00200 | ms/batch 160.70 | loss  4.36 | ppl    78.63 | bpc    6.297
| epoch   1 |    60/  100 batches | lr 0.00200 | ms/batch 161.47 | loss  4.36 | ppl    78.44 | bpc    6.294
| epoch   1 |    70/  100 batches | lr 0.00200 | ms/batch 153.49 | loss  4.38 | ppl    79.52 | bpc    6.313
| epoch   1 |    80/  100 batches | lr 0.00200 | ms/batch 160.59 | loss  4.37 | ppl    79.22 | bpc    6.308
| epoch   1 |    90/  100 batches | lr 0.00200 | ms/batch 156.66 | loss  4.36 | ppl    78.15 | bpc    6.288
| epoch   1 |   100/  100 batches | lr 0.00200 | ms/batch 160.96 | loss  4.36 | ppl    78.24 | bpc    6.290
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 23.80s | valid loss  4.34 | valid ppl    76.76 | valid bpc    6.262
-----------------------------------------------------------------------------------------
Saving model (new best validation)
| epoch   2 |    10/  100 batches | lr 0.00200 | ms/batch 167.30 | loss  4.81 | ppl   122.49 | bpc    6.936
| epoch   2 |    20/  100 batches | lr 0.00200 | ms/batch 142.55 | loss  4.39 | ppl    80.65 | bpc    6.334
| epoch   2 |    30/  100 batches | lr 0.00200 | ms/batch 161.26 | loss  4.38 | ppl    79.70 | bpc    6.316
| epoch   2 |    40/  100 batches | lr 0.00200 | ms/batch 154.93 | loss  4.36 | ppl    78.63 | bpc    6.297
| epoch   2 |    50/  100 batches | lr 0.00200 | ms/batch 159.65 | loss  4.35 | ppl    77.59 | bpc    6.278
| epoch   2 |    60/  100 batches | lr 0.00200 | ms/batch 154.15 | loss  4.34 | ppl    76.96 | bpc    6.266
| epoch   2 |    70/  100 batches | lr 0.00200 | ms/batch 167.88 | loss  4.36 | ppl    78.14 | bpc    6.288
| epoch   2 |    80/  100 batches | lr 0.00200 | ms/batch 165.84 | loss  4.36 | ppl    78.41 | bpc    6.293
| epoch   2 |    90/  100 batches | lr 0.00200 | ms/batch 153.50 | loss  4.35 | ppl    77.80 | bpc    6.282
| epoch   2 |   100/  100 batches | lr 0.00200 | ms/batch 160.91 | loss  4.35 | ppl    77.71 | bpc    6.280
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 24.01s | valid loss  4.34 | valid ppl    76.72 | valid bpc    6.261
-----------------------------------------------------------------------------------------
Saving model (new best validation)
| epoch   3 |    10/  100 batches | lr 0.00200 | ms/batch 169.16 | loss  4.81 | ppl   122.31 | bpc    6.934
| epoch   3 |    20/  100 batches | lr 0.00200 | ms/batch 160.02 | loss  4.38 | ppl    79.58 | bpc    6.314
| epoch   3 |    30/  100 batches | lr 0.00200 | ms/batch 154.45 | loss  4.38 | ppl    79.69 | bpc    6.316
| epoch   3 |    40/  100 batches | lr 0.00200 | ms/batch 160.73 | loss  4.38 | ppl    79.54 | bpc    6.314
| epoch   3 |    50/  100 batches | lr 0.00200 | ms/batch 161.58 | loss  4.37 | ppl    79.03 | bpc    6.304
| epoch   3 |    60/  100 batches | lr 0.00200 | ms/batch 155.41 | loss  4.37 | ppl    78.74 | bpc    6.299
| epoch   3 |    70/  100 batches | lr 0.00200 | ms/batch 160.70 | loss  4.37 | ppl    79.27 | bpc    6.309
| epoch   3 |    80/  100 batches | lr 0.00200 | ms/batch 161.72 | loss  4.38 | ppl    79.53 | bpc    6.313
| epoch   3 |    90/  100 batches | lr 0.00200 | ms/batch 172.35 | loss  4.37 | ppl    79.42 | bpc    6.311
| epoch   3 |   100/  100 batches | lr 0.00200 | ms/batch 166.65 | loss  4.37 | ppl    79.10 | bpc    6.306
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 24.30s | valid loss  4.36 | valid ppl    78.58 | valid bpc    6.296
-----------------------------------------------------------------------------------------
| epoch   4 |    10/  100 batches | lr 0.00200 | ms/batch 168.91 | loss  4.82 | ppl   123.71 | bpc    6.951
| epoch   4 |    20/  100 batches | lr 0.00200 | ms/batch 160.66 | loss  4.39 | ppl    80.40 | bpc    6.329
| epoch   4 |    30/  100 batches | lr 0.00200 | ms/batch 160.71 | loss  4.38 | ppl    80.02 | bpc    6.322
| epoch   4 |    40/  100 batches | lr 0.00200 | ms/batch 154.95 | loss  4.38 | ppl    79.86 | bpc    6.319
| epoch   4 |    50/  100 batches | lr 0.00200 | ms/batch 161.55 | loss  4.38 | ppl    79.64 | bpc    6.315
| epoch   4 |    60/  100 batches | lr 0.00200 | ms/batch 154.48 | loss  4.37 | ppl    79.31 | bpc    6.309
| epoch   4 |    70/  100 batches | lr 0.00200 | ms/batch 155.56 | loss  4.38 | ppl    79.82 | bpc    6.319
| epoch   4 |    80/  100 batches | lr 0.00200 | ms/batch 161.42 | loss  4.38 | ppl    79.79 | bpc    6.318
| epoch   4 |    90/  100 batches | lr 0.00200 | ms/batch 146.37 | loss  4.38 | ppl    79.74 | bpc    6.317
| epoch   4 |   100/  100 batches | lr 0.00200 | ms/batch 160.66 | loss  4.37 | ppl    79.40 | bpc    6.311
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 24.34s | valid loss  4.37 | valid ppl    78.83 | valid bpc    6.301
-----------------------------------------------------------------------------------------
enlarge model: emsize=42, nhid=42 (discard_highest_losses=0.075)
Args: Namespace(accumulate=1, alpha=2, batch_size=32, beta=1, bptt=1024, clip=0.25, cooldown=None, cuda=True, data='data/enwik8/', discard_highest_losses=0.1, dropout=0.1, dropoute=0.1, dropouth=0.1, dropouti=0.1, emsize=64, enlarge_model_every_n_epochs=5, epochs=20, log_interval=10, lr=0.002, max_steps_per_epoch=100, model='LSTM', nhid=64, nlayers=4, nonmono=5, optimizer='lamb', resume='', save='ENWIK8.pt', seed=5512, tied=True, warmup=800, wdecay=1.2e-06, wdrop=0.0, when=[-1])
Model total parameters: 81356
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
| epoch   5 |    10/  100 batches | lr 0.00200 | ms/batch 176.52 | loss  4.87 | ppl   130.29 | bpc    7.026
| epoch   5 |    20/  100 batches | lr 0.00200 | ms/batch 175.55 | loss  4.38 | ppl    79.79 | bpc    6.318
| epoch   5 |    30/  100 batches | lr 0.00200 | ms/batch 186.16 | loss  4.38 | ppl    79.53 | bpc    6.313
| epoch   5 |    40/  100 batches | lr 0.00200 | ms/batch 170.71 | loss  4.38 | ppl    79.57 | bpc    6.314
| epoch   5 |    50/  100 batches | lr 0.00200 | ms/batch 175.62 | loss  4.38 | ppl    79.75 | bpc    6.317
| epoch   5 |    60/  100 batches | lr 0.00200 | ms/batch 168.17 | loss  4.38 | ppl    79.66 | bpc    6.316
| epoch   5 |    70/  100 batches | lr 0.00200 | ms/batch 168.74 | loss  4.38 | ppl    80.24 | bpc    6.326
| epoch   5 |    80/  100 batches | lr 0.00200 | ms/batch 175.53 | loss  4.38 | ppl    79.97 | bpc    6.321
| epoch   5 |    90/  100 batches | lr 0.00200 | ms/batch 175.57 | loss  4.38 | ppl    80.05 | bpc    6.323
| epoch   5 |   100/  100 batches | lr 0.00200 | ms/batch 174.74 | loss  4.38 | ppl    80.08 | bpc    6.323
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 26.21s | valid loss  4.38 | valid ppl    79.77 | valid bpc    6.318
-----------------------------------------------------------------------------------------
| epoch   6 |    10/  100 batches | lr 0.00200 | ms/batch 184.79 | loss  4.82 | ppl   123.58 | bpc    6.949
| epoch   6 |    20/  100 batches | lr 0.00200 | ms/batch 173.42 | loss  4.38 | ppl    80.13 | bpc    6.324
| epoch   6 |    30/  100 batches | lr 0.00200 | ms/batch 175.00 | loss  4.38 | ppl    79.72 | bpc    6.317
| epoch   6 |    40/  100 batches | lr 0.00200 | ms/batch 169.41 | loss  4.38 | ppl    79.83 | bpc    6.319
| epoch   6 |    50/  100 batches | lr 0.00200 | ms/batch 173.98 | loss  4.38 | ppl    79.65 | bpc    6.316
| epoch   6 |    60/  100 batches | lr 0.00200 | ms/batch 175.27 | loss  4.37 | ppl    79.32 | bpc    6.310
| epoch   6 |    70/  100 batches | lr 0.00200 | ms/batch 167.61 | loss  4.38 | ppl    79.52 | bpc    6.313
| epoch   6 |    80/  100 batches | lr 0.00200 | ms/batch 173.86 | loss  4.37 | ppl    79.23 | bpc    6.308
| epoch   6 |    90/  100 batches | lr 0.00200 | ms/batch 169.63 | loss  4.37 | ppl    79.11 | bpc    6.306
| epoch   6 |   100/  100 batches | lr 0.00200 | ms/batch 175.33 | loss  4.36 | ppl    78.62 | bpc    6.297
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 26.10s | valid loss  4.35 | valid ppl    77.69 | valid bpc    6.280
-----------------------------------------------------------------------------------------
| epoch   7 |    10/  100 batches | lr 0.00200 | ms/batch 182.70 | loss  4.79 | ppl   120.19 | bpc    6.909
| epoch   7 |    20/  100 batches | lr 0.00200 | ms/batch 175.44 | loss  4.35 | ppl    77.64 | bpc    6.279
| epoch   7 |    30/  100 batches | lr 0.00200 | ms/batch 168.49 | loss  4.34 | ppl    76.33 | bpc    6.254
| epoch   7 |    40/  100 batches | lr 0.00200 | ms/batch 175.77 | loss  4.33 | ppl    75.90 | bpc    6.246
| epoch   7 |    50/  100 batches | lr 0.00200 | ms/batch 168.99 | loss  4.31 | ppl    74.74 | bpc    6.224
| epoch   7 |    60/  100 batches | lr 0.00200 | ms/batch 175.27 | loss  4.29 | ppl    73.15 | bpc    6.193
| epoch   7 |    70/  100 batches | lr 0.00200 | ms/batch 162.28 | loss  4.28 | ppl    72.15 | bpc    6.173
| epoch   7 |    80/  100 batches | lr 0.00200 | ms/batch 160.92 | loss  4.26 | ppl    70.48 | bpc    6.139
| epoch   7 |    90/  100 batches | lr 0.00200 | ms/batch 174.44 | loss  4.23 | ppl    69.03 | bpc    6.109
| epoch   7 |   100/  100 batches | lr 0.00200 | ms/batch 168.54 | loss  4.21 | ppl    67.57 | bpc    6.078
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 25.86s | valid loss  4.18 | valid ppl    65.61 | valid bpc    6.036
-----------------------------------------------------------------------------------------
Saving model (new best validation)
| epoch   8 |    10/  100 batches | lr 0.00200 | ms/batch 181.33 | loss  4.62 | ppl   101.43 | bpc    6.664
| epoch   8 |    20/  100 batches | lr 0.00200 | ms/batch 168.07 | loss  4.20 | ppl    66.80 | bpc    6.062
| epoch   8 |    30/  100 batches | lr 0.00200 | ms/batch 167.94 | loss  4.19 | ppl    65.71 | bpc    6.038
| epoch   8 |    40/  100 batches | lr 0.00200 | ms/batch 168.87 | loss  4.17 | ppl    64.98 | bpc    6.022
| epoch   8 |    50/  100 batches | lr 0.00200 | ms/batch 175.24 | loss  4.16 | ppl    63.98 | bpc    6.000
| epoch   8 |    60/  100 batches | lr 0.00200 | ms/batch 176.24 | loss  4.13 | ppl    62.45 | bpc    5.965
| epoch   8 |    70/  100 batches | lr 0.00200 | ms/batch 174.58 | loss  4.13 | ppl    61.99 | bpc    5.954
| epoch   8 |    80/  100 batches | lr 0.00200 | ms/batch 175.13 | loss  4.11 | ppl    60.98 | bpc    5.930
| epoch   8 |    90/  100 batches | lr 0.00200 | ms/batch 176.07 | loss  4.10 | ppl    60.10 | bpc    5.909
| epoch   8 |   100/  100 batches | lr 0.00200 | ms/batch 169.29 | loss  4.09 | ppl    59.50 | bpc    5.895
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 26.05s | valid loss  4.06 | valid ppl    57.69 | valid bpc    5.850
-----------------------------------------------------------------------------------------
Saving model (new best validation)
| epoch   9 |    10/  100 batches | lr 0.00200 | ms/batch 184.40 | loss  4.50 | ppl    89.68 | bpc    6.487
| epoch   9 |    20/  100 batches | lr 0.00200 | ms/batch 168.12 | loss  4.10 | ppl    60.36 | bpc    5.915
| epoch   9 |    30/  100 batches | lr 0.00200 | ms/batch 168.28 | loss  4.09 | ppl    59.75 | bpc    5.901
| epoch   9 |    40/  100 batches | lr 0.00200 | ms/batch 174.32 | loss  4.09 | ppl    59.96 | bpc    5.906
| epoch   9 |    50/  100 batches | lr 0.00200 | ms/batch 168.47 | loss  4.08 | ppl    59.24 | bpc    5.889
| epoch   9 |    60/  100 batches | lr 0.00200 | ms/batch 175.51 | loss  4.06 | ppl    57.72 | bpc    5.851
| epoch   9 |    70/  100 batches | lr 0.00200 | ms/batch 174.62 | loss  4.06 | ppl    57.88 | bpc    5.855
| epoch   9 |    80/  100 batches | lr 0.00200 | ms/batch 183.38 | loss  4.04 | ppl    57.07 | bpc    5.835
| epoch   9 |    90/  100 batches | lr 0.00200 | ms/batch 191.07 | loss  4.04 | ppl    56.70 | bpc    5.825
| epoch   9 |   100/  100 batches | lr 0.00200 | ms/batch 174.12 | loss  4.04 | ppl    56.57 | bpc    5.822
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 26.51s | valid loss  4.01 | valid ppl    54.88 | valid bpc    5.778
-----------------------------------------------------------------------------------------
Saving model (new best validation)
enlarge model: emsize=64, nhid=64 (discard_highest_losses=0.05)
Args: Namespace(accumulate=1, alpha=2, batch_size=32, beta=1, bptt=1024, clip=0.25, cooldown=None, cuda=True, data='data/enwik8/', discard_highest_losses=0.1, dropout=0.1, dropoute=0.1, dropouth=0.1, dropouti=0.1, emsize=64, enlarge_model_every_n_epochs=5, epochs=20, log_interval=10, lr=0.002, max_steps_per_epoch=100, model='LSTM', nhid=64, nlayers=4, nonmono=5, optimizer='lamb', resume='', save='ENWIK8.pt', seed=5512, tied=True, warmup=800, wdecay=1.2e-06, wdrop=0.0, when=[-1])
Model total parameters: 178772
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
| epoch  10 |    10/  100 batches | lr 0.00200 | ms/batch 181.10 | loss  4.94 | ppl   139.78 | bpc    7.127
| epoch  10 |    20/  100 batches | lr 0.00200 | ms/batch 195.45 | loss  4.38 | ppl    79.71 | bpc    6.317
| epoch  10 |    30/  100 batches | lr 0.00200 | ms/batch 180.16 | loss  4.37 | ppl    78.74 | bpc    6.299
| epoch  10 |    40/  100 batches | lr 0.00200 | ms/batch 177.63 | loss  4.37 | ppl    79.02 | bpc    6.304
| epoch  10 |    50/  100 batches | lr 0.00200 | ms/batch 188.30 | loss  4.36 | ppl    78.27 | bpc    6.290
| epoch  10 |    60/  100 batches | lr 0.00200 | ms/batch 188.32 | loss  4.35 | ppl    77.24 | bpc    6.271
| epoch  10 |    70/  100 batches | lr 0.00200 | ms/batch 188.82 | loss  4.34 | ppl    76.83 | bpc    6.264
| epoch  10 |    80/  100 batches | lr 0.00200 | ms/batch 173.03 | loss  4.31 | ppl    74.25 | bpc    6.214
| epoch  10 |    90/  100 batches | lr 0.00200 | ms/batch 174.64 | loss  4.28 | ppl    72.03 | bpc    6.171
| epoch  10 |   100/  100 batches | lr 0.00200 | ms/batch 172.92 | loss  4.23 | ppl    68.80 | bpc    6.104
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 26.37s | valid loss  4.18 | valid ppl    65.50 | valid bpc    6.033
-----------------------------------------------------------------------------------------
| epoch  11 |    10/  100 batches | lr 0.00200 | ms/batch 184.94 | loss  4.62 | ppl   101.00 | bpc    6.658
| epoch  11 |    20/  100 batches | lr 0.00200 | ms/batch 167.09 | loss  4.19 | ppl    66.17 | bpc    6.048
| epoch  11 |    30/  100 batches | lr 0.00200 | ms/batch 161.63 | loss  4.17 | ppl    64.56 | bpc    6.013
| epoch  11 |    40/  100 batches | lr 0.00200 | ms/batch 173.58 | loss  4.15 | ppl    63.68 | bpc    5.993
| epoch  11 |    50/  100 batches | lr 0.00200 | ms/batch 166.96 | loss  4.14 | ppl    62.68 | bpc    5.970
| epoch  11 |    60/  100 batches | lr 0.00200 | ms/batch 175.10 | loss  4.11 | ppl    60.97 | bpc    5.930
| epoch  11 |    70/  100 batches | lr 0.00200 | ms/batch 173.56 | loss  4.10 | ppl    60.44 | bpc    5.917
| epoch  11 |    80/  100 batches | lr 0.00200 | ms/batch 167.23 | loss  4.10 | ppl    60.64 | bpc    5.922
| epoch  11 |    90/  100 batches | lr 0.00200 | ms/batch 173.96 | loss  4.10 | ppl    60.15 | bpc    5.910
| epoch  11 |   100/  100 batches | lr 0.00200 | ms/batch 173.37 | loss  4.09 | ppl    59.47 | bpc    5.894
-----------------------------------------------------------------------------------------
| end of epoch  12 | time: 25.33s | valid loss  4.05 | valid ppl    57.44 | valid bpc    5.844
-----------------------------------------------------------------------------------------
| epoch  12 |    10/  100 batches | lr 0.00200 | ms/batch 177.07 | loss  4.49 | ppl    89.54 | bpc    6.484
| epoch  12 |    20/  100 batches | lr 0.00200 | ms/batch 159.16 | loss  4.11 | ppl    61.24 | bpc    5.936
| epoch  12 |    30/  100 batches | lr 0.00200 | ms/batch 174.79 | loss  4.11 | ppl    60.98 | bpc    5.930
| epoch  12 |    40/  100 batches | lr 0.00200 | ms/batch 188.67 | loss  4.10 | ppl    60.32 | bpc    5.915
| epoch  12 |    50/  100 batches | lr 0.00200 | ms/batch 174.26 | loss  4.09 | ppl    59.73 | bpc    5.900
| epoch  12 |    60/  100 batches | lr 0.00200 | ms/batch 173.99 | loss  4.07 | ppl    58.85 | bpc    5.879
| epoch  12 |    70/  100 batches | lr 0.00200 | ms/batch 167.08 | loss  4.07 | ppl    58.63 | bpc    5.874
| epoch  12 |    80/  100 batches | lr 0.00200 | ms/batch 173.19 | loss  4.07 | ppl    58.73 | bpc    5.876
| epoch  12 |    90/  100 batches | lr 0.00200 | ms/batch 173.10 | loss  4.07 | ppl    58.80 | bpc    5.878
| epoch  12 |   100/  100 batches | lr 0.00200 | ms/batch 173.34 | loss  4.07 | ppl    58.61 | bpc    5.873
-----------------------------------------------------------------------------------------
| end of epoch  13 | time: 25.50s | valid loss  4.04 | valid ppl    56.85 | valid bpc    5.829
-----------------------------------------------------------------------------------------
| epoch  13 |    10/  100 batches | lr 0.00200 | ms/batch 175.73 | loss  4.48 | ppl    88.52 | bpc    6.468
| epoch  13 |    20/  100 batches | lr 0.00200 | ms/batch 172.98 | loss  4.10 | ppl    60.42 | bpc    5.917
| epoch  13 |    30/  100 batches | lr 0.00200 | ms/batch 160.95 | loss  4.10 | ppl    60.16 | bpc    5.911
| epoch  13 |    40/  100 batches | lr 0.00200 | ms/batch 177.12 | loss  4.09 | ppl    59.96 | bpc    5.906
| epoch  13 |    50/  100 batches | lr 0.00200 | ms/batch 173.76 | loss  4.09 | ppl    59.46 | bpc    5.894
| epoch  13 |    60/  100 batches | lr 0.00200 | ms/batch 181.27 | loss  4.07 | ppl    58.64 | bpc    5.874
| epoch  13 |    70/  100 batches | lr 0.00200 | ms/batch 173.60 | loss  4.07 | ppl    58.61 | bpc    5.873
| epoch  13 |    80/  100 batches | lr 0.00200 | ms/batch 173.06 | loss  4.08 | ppl    58.93 | bpc    5.881
| epoch  13 |    90/  100 batches | lr 0.00200 | ms/batch 167.05 | loss  4.08 | ppl    59.26 | bpc    5.889
| epoch  13 |   100/  100 batches | lr 0.00200 | ms/batch 166.84 | loss  4.08 | ppl    58.95 | bpc    5.881
-----------------------------------------------------------------------------------------
| end of epoch  14 | time: 25.37s | valid loss  4.05 | valid ppl    57.55 | valid bpc    5.847
-----------------------------------------------------------------------------------------
| epoch  14 |    10/  100 batches | lr 0.00200 | ms/batch 184.26 | loss  4.49 | ppl    88.96 | bpc    6.475
| epoch  14 |    20/  100 batches | lr 0.00200 | ms/batch 167.79 | loss  4.10 | ppl    60.49 | bpc    5.919
| epoch  14 |    30/  100 batches | lr 0.00200 | ms/batch 166.89 | loss  4.10 | ppl    60.34 | bpc    5.915
| epoch  14 |    40/  100 batches | lr 0.00200 | ms/batch 166.79 | loss  4.10 | ppl    60.24 | bpc    5.913
| epoch  14 |    50/  100 batches | lr 0.00200 | ms/batch 168.73 | loss  4.09 | ppl    59.93 | bpc    5.905
| epoch  14 |    60/  100 batches | lr 0.00200 | ms/batch 167.74 | loss  4.08 | ppl    59.43 | bpc    5.893
| epoch  14 |    70/  100 batches | lr 0.00200 | ms/batch 174.70 | loss  4.08 | ppl    59.30 | bpc    5.890
| epoch  14 |    80/  100 batches | lr 0.00200 | ms/batch 173.51 | loss  4.08 | ppl    59.33 | bpc    5.891
| epoch  14 |    90/  100 batches | lr 0.00200 | ms/batch 168.50 | loss  4.08 | ppl    59.42 | bpc    5.893
| epoch  14 |   100/  100 batches | lr 0.00200 | ms/batch 174.77 | loss  4.08 | ppl    59.42 | bpc    5.893
-----------------------------------------------------------------------------------------
| end of epoch  15 | time: 25.34s | valid loss  4.06 | valid ppl    57.98 | valid bpc    5.858
-----------------------------------------------------------------------------------------
enlarge model: emsize=85, nhid=85 (discard_highest_losses=0.025)
Args: Namespace(accumulate=1, alpha=2, batch_size=32, beta=1, bptt=1024, clip=0.25, cooldown=None, cuda=True, data='data/enwik8/', discard_highest_losses=0.1, dropout=0.1, dropoute=0.1, dropouth=0.1, dropouti=0.1, emsize=64, enlarge_model_every_n_epochs=5, epochs=20, log_interval=10, lr=0.002, max_steps_per_epoch=100, model='LSTM', nhid=64, nlayers=4, nonmono=5, optimizer='lamb', resume='', save='ENWIK8.pt', seed=5512, tied=True, warmup=800, wdecay=1.2e-06, wdrop=0.0, when=[-1])
Model total parameters: 306977
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
| epoch  15 |    10/  100 batches | lr 0.00200 | ms/batch 217.82 | loss  5.00 | ppl   148.16 | bpc    7.211
| epoch  15 |    20/  100 batches | lr 0.00200 | ms/batch 201.09 | loss  4.50 | ppl    90.24 | bpc    6.496
| epoch  15 |    30/  100 batches | lr 0.00200 | ms/batch 199.14 | loss  4.47 | ppl    87.02 | bpc    6.443
| epoch  15 |    40/  100 batches | lr 0.00200 | ms/batch 199.46 | loss  4.47 | ppl    87.56 | bpc    6.452
| epoch  15 |    50/  100 batches | lr 0.00200 | ms/batch 184.53 | loss  4.45 | ppl    85.98 | bpc    6.426
| epoch  15 |    60/  100 batches | lr 0.00200 | ms/batch 191.28 | loss  4.42 | ppl    83.40 | bpc    6.382
| epoch  15 |    70/  100 batches | lr 0.00200 | ms/batch 198.09 | loss  4.44 | ppl    84.36 | bpc    6.398
| epoch  15 |    80/  100 batches | lr 0.00200 | ms/batch 199.60 | loss  4.42 | ppl    83.34 | bpc    6.381
| epoch  15 |    90/  100 batches | lr 0.00200 | ms/batch 190.70 | loss  4.42 | ppl    82.89 | bpc    6.373
| epoch  15 |   100/  100 batches | lr 0.00200 | ms/batch 199.27 | loss  4.41 | ppl    82.26 | bpc    6.362
-----------------------------------------------------------------------------------------
| end of epoch  16 | time: 29.76s | valid loss  4.38 | valid ppl    80.14 | valid bpc    6.324
-----------------------------------------------------------------------------------------
| epoch  16 |    10/  100 batches | lr 0.00200 | ms/batch 212.61 | loss  4.86 | ppl   128.61 | bpc    7.007
| epoch  16 |    20/  100 batches | lr 0.00200 | ms/batch 200.53 | loss  4.43 | ppl    83.63 | bpc    6.386
| epoch  16 |    30/  100 batches | lr 0.00200 | ms/batch 200.02 | loss  4.39 | ppl    80.93 | bpc    6.339
| epoch  16 |    40/  100 batches | lr 0.00200 | ms/batch 204.92 | loss  4.40 | ppl    81.45 | bpc    6.348
| epoch  16 |    50/  100 batches | lr 0.00200 | ms/batch 200.55 | loss  4.39 | ppl    80.30 | bpc    6.327
| epoch  16 |    60/  100 batches | lr 0.00200 | ms/batch 207.55 | loss  4.36 | ppl    78.54 | bpc    6.295
| epoch  16 |    70/  100 batches | lr 0.00200 | ms/batch 210.78 | loss  4.37 | ppl    79.29 | bpc    6.309
| epoch  16 |    80/  100 batches | lr 0.00200 | ms/batch 198.97 | loss  4.35 | ppl    77.31 | bpc    6.273
| epoch  16 |    90/  100 batches | lr 0.00200 | ms/batch 193.61 | loss  4.35 | ppl    77.41 | bpc    6.274
| epoch  16 |   100/  100 batches | lr 0.00200 | ms/batch 191.68 | loss  4.34 | ppl    76.86 | bpc    6.264
-----------------------------------------------------------------------------------------
| end of epoch  17 | time: 30.19s | valid loss  4.33 | valid ppl    75.96 | valid bpc    6.247
-----------------------------------------------------------------------------------------
| epoch  17 |    10/  100 batches | lr 0.00200 | ms/batch 205.04 | loss  4.79 | ppl   120.46 | bpc    6.912
| epoch  17 |    20/  100 batches | lr 0.00200 | ms/batch 201.07 | loss  4.38 | ppl    79.83 | bpc    6.319
| epoch  17 |    30/  100 batches | lr 0.00200 | ms/batch 202.68 | loss  4.35 | ppl    77.42 | bpc    6.275
| epoch  17 |    40/  100 batches | lr 0.00200 | ms/batch 202.79 | loss  4.35 | ppl    77.51 | bpc    6.276
| epoch  17 |    50/  100 batches | lr 0.00200 | ms/batch 204.16 | loss  4.33 | ppl    75.82 | bpc    6.245
| epoch  17 |    60/  100 batches | lr 0.00200 | ms/batch 202.98 | loss  4.32 | ppl    75.35 | bpc    6.236
| epoch  17 |    70/  100 batches | lr 0.00200 | ms/batch 205.51 | loss  4.34 | ppl    76.93 | bpc    6.265
| epoch  17 |    80/  100 batches | lr 0.00200 | ms/batch 197.21 | loss  4.33 | ppl    75.99 | bpc    6.248
| epoch  17 |    90/  100 batches | lr 0.00200 | ms/batch 194.87 | loss  4.32 | ppl    75.56 | bpc    6.239
| epoch  17 |   100/  100 batches | lr 0.00200 | ms/batch 200.44 | loss  4.31 | ppl    74.80 | bpc    6.225
-----------------------------------------------------------------------------------------
| end of epoch  18 | time: 30.22s | valid loss  4.31 | valid ppl    74.53 | valid bpc    6.220
-----------------------------------------------------------------------------------------
| epoch  18 |    10/  100 batches | lr 0.00200 | ms/batch 210.40 | loss  4.78 | ppl   118.77 | bpc    6.892
| epoch  18 |    20/  100 batches | lr 0.00200 | ms/batch 204.65 | loss  4.39 | ppl    80.30 | bpc    6.327
| epoch  18 |    30/  100 batches | lr 0.00200 | ms/batch 211.81 | loss  4.36 | ppl    78.60 | bpc    6.296
| epoch  18 |    40/  100 batches | lr 0.00200 | ms/batch 203.92 | loss  4.37 | ppl    78.87 | bpc    6.301
| epoch  18 |    50/  100 batches | lr 0.00200 | ms/batch 194.06 | loss  4.35 | ppl    77.54 | bpc    6.277
| epoch  18 |    60/  100 batches | lr 0.00200 | ms/batch 202.67 | loss  4.34 | ppl    76.81 | bpc    6.263
| epoch  18 |    70/  100 batches | lr 0.00200 | ms/batch 195.84 | loss  4.35 | ppl    77.73 | bpc    6.280
| epoch  18 |    80/  100 batches | lr 0.00200 | ms/batch 195.49 | loss  4.36 | ppl    77.93 | bpc    6.284
| epoch  18 |    90/  100 batches | lr 0.00200 | ms/batch 208.08 | loss  4.35 | ppl    77.80 | bpc    6.282
| epoch  18 |   100/  100 batches | lr 0.00200 | ms/batch 201.50 | loss  4.33 | ppl    76.14 | bpc    6.251
-----------------------------------------------------------------------------------------
| end of epoch  19 | time: 30.27s | valid loss  4.32 | valid ppl    75.45 | valid bpc    6.237
-----------------------------------------------------------------------------------------
| epoch  19 |    10/  100 batches | lr 0.00200 | ms/batch 205.86 | loss  4.81 | ppl   123.05 | bpc    6.943
| epoch  19 |    20/  100 batches | lr 0.00200 | ms/batch 196.50 | loss  4.43 | ppl    84.27 | bpc    6.397
| epoch  19 |    30/  100 batches | lr 0.00200 | ms/batch 188.49 | loss  4.42 | ppl    83.41 | bpc    6.382
| epoch  19 |    40/  100 batches | lr 0.00200 | ms/batch 195.47 | loss  4.42 | ppl    83.05 | bpc    6.376
| epoch  19 |    50/  100 batches | lr 0.00200 | ms/batch 189.37 | loss  4.40 | ppl    81.76 | bpc    6.353
| epoch  19 |    60/  100 batches | lr 0.00200 | ms/batch 189.37 | loss  4.39 | ppl    80.35 | bpc    6.328
| epoch  19 |    70/  100 batches | lr 0.00200 | ms/batch 196.44 | loss  4.41 | ppl    82.27 | bpc    6.362
| epoch  19 |    80/  100 batches | lr 0.00200 | ms/batch 197.71 | loss  4.43 | ppl    84.18 | bpc    6.395
| epoch  19 |    90/  100 batches | lr 0.00200 | ms/batch 201.13 | loss  4.43 | ppl    84.06 | bpc    6.393
| epoch  19 |   100/  100 batches | lr 0.00200 | ms/batch 192.92 | loss  4.42 | ppl    82.99 | bpc    6.375
-----------------------------------------------------------------------------------------
| end of epoch  20 | time: 29.48s | valid loss  4.40 | valid ppl    81.72 | valid bpc    6.353
-----------------------------------------------------------------------------------------
load mode from: ENWIK8.pt
Traceback (most recent call last):
  File "/home/arne/projects/dfki/sha-rnn/main.py", line 508, in <module>
    main()
  File "/home/arne/projects/dfki/sha-rnn/main.py", line 493, in main
    criterion = model_load(args.save, model)
  File "/home/arne/projects/dfki/sha-rnn/main.py", line 34, in model_load
    model.load_state_dict(d, strict=False)
  File "/home/arne/miniconda3/envs/incplex/lib/python3.8/site-packages/torch/nn/modules/module.py", line 829, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for SHARNN:
	size mismatch for blocks.0.ff.linear1.weight: copying a param with shape torch.Size([42, 42]) from checkpoint, the shape in current model is torch.Size([85, 85]).
	size mismatch for blocks.0.ff.linear1.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnstart.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnstart.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnmid.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnmid.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnmem.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnmem.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnout.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnout.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnff.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnff.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnxff.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.lnxff.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.0.rnn.weight_ih_l0: copying a param with shape torch.Size([168, 42]) from checkpoint, the shape in current model is torch.Size([340, 85]).
	size mismatch for blocks.0.rnn.weight_hh_l0: copying a param with shape torch.Size([168, 42]) from checkpoint, the shape in current model is torch.Size([340, 85]).
	size mismatch for blocks.0.rnn.bias_ih_l0: copying a param with shape torch.Size([168]) from checkpoint, the shape in current model is torch.Size([340]).
	size mismatch for blocks.0.rnn.bias_hh_l0: copying a param with shape torch.Size([168]) from checkpoint, the shape in current model is torch.Size([340]).
	size mismatch for blocks.1.ff.linear1.weight: copying a param with shape torch.Size([42, 42]) from checkpoint, the shape in current model is torch.Size([85, 85]).
	size mismatch for blocks.1.ff.linear1.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnstart.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnstart.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnmid.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnmid.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnmem.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnmem.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnout.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnout.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnff.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnff.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnxff.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.lnxff.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.1.rnn.weight_ih_l0: copying a param with shape torch.Size([168, 42]) from checkpoint, the shape in current model is torch.Size([340, 85]).
	size mismatch for blocks.1.rnn.weight_hh_l0: copying a param with shape torch.Size([168, 42]) from checkpoint, the shape in current model is torch.Size([340, 85]).
	size mismatch for blocks.1.rnn.bias_ih_l0: copying a param with shape torch.Size([168]) from checkpoint, the shape in current model is torch.Size([340]).
	size mismatch for blocks.1.rnn.bias_hh_l0: copying a param with shape torch.Size([168]) from checkpoint, the shape in current model is torch.Size([340]).
	size mismatch for blocks.2.attn.qs: copying a param with shape torch.Size([1, 1, 42]) from checkpoint, the shape in current model is torch.Size([1, 1, 85]).
	size mismatch for blocks.2.attn.ks: copying a param with shape torch.Size([1, 1, 42]) from checkpoint, the shape in current model is torch.Size([1, 1, 85]).
	size mismatch for blocks.2.attn.vs: copying a param with shape torch.Size([1, 1, 42]) from checkpoint, the shape in current model is torch.Size([1, 1, 85]).
	size mismatch for blocks.2.attn.qkvs: copying a param with shape torch.Size([1, 3, 42]) from checkpoint, the shape in current model is torch.Size([1, 3, 85]).
	size mismatch for blocks.2.attn.r_gate: copying a param with shape torch.Size([1, 1, 42]) from checkpoint, the shape in current model is torch.Size([1, 1, 85]).
	size mismatch for blocks.2.attn.q.weight: copying a param with shape torch.Size([42, 42]) from checkpoint, the shape in current model is torch.Size([85, 85]).
	size mismatch for blocks.2.attn.q.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.attn.qln.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.attn.qln.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.attn.vq.l1.weight: copying a param with shape torch.Size([84, 42]) from checkpoint, the shape in current model is torch.Size([170, 85]).
	size mismatch for blocks.2.attn.vq.l1.bias: copying a param with shape torch.Size([84]) from checkpoint, the shape in current model is torch.Size([170]).
	size mismatch for blocks.2.ff.linear1.weight: copying a param with shape torch.Size([42, 42]) from checkpoint, the shape in current model is torch.Size([85, 85]).
	size mismatch for blocks.2.ff.linear1.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnstart.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnstart.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnmid.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnmid.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnmem.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnmem.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnout.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnout.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnff.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnff.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnxff.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.lnxff.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.2.rnn.weight_ih_l0: copying a param with shape torch.Size([168, 42]) from checkpoint, the shape in current model is torch.Size([340, 85]).
	size mismatch for blocks.2.rnn.weight_hh_l0: copying a param with shape torch.Size([168, 42]) from checkpoint, the shape in current model is torch.Size([340, 85]).
	size mismatch for blocks.2.rnn.bias_ih_l0: copying a param with shape torch.Size([168]) from checkpoint, the shape in current model is torch.Size([340]).
	size mismatch for blocks.2.rnn.bias_hh_l0: copying a param with shape torch.Size([168]) from checkpoint, the shape in current model is torch.Size([340]).
	size mismatch for blocks.3.ff.linear1.weight: copying a param with shape torch.Size([42, 42]) from checkpoint, the shape in current model is torch.Size([85, 85]).
	size mismatch for blocks.3.ff.linear1.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnstart.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnstart.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnmid.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnmid.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnmem.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnmem.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnout.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnout.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnff.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnff.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnxff.weight: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.lnxff.bias: copying a param with shape torch.Size([42]) from checkpoint, the shape in current model is torch.Size([85]).
	size mismatch for blocks.3.rnn.weight_ih_l0: copying a param with shape torch.Size([168, 42]) from checkpoint, the shape in current model is torch.Size([340, 85]).
	size mismatch for blocks.3.rnn.weight_hh_l0: copying a param with shape torch.Size([168, 42]) from checkpoint, the shape in current model is torch.Size([340, 85]).
	size mismatch for blocks.3.rnn.bias_ih_l0: copying a param with shape torch.Size([168]) from checkpoint, the shape in current model is torch.Size([340]).
	size mismatch for blocks.3.rnn.bias_hh_l0: copying a param with shape torch.Size([168]) from checkpoint, the shape in current model is torch.Size([340]).
	size mismatch for encoder.weight: copying a param with shape torch.Size([205, 42]) from checkpoint, the shape in current model is torch.Size([205, 85]).
	size mismatch for decoder.weight: copying a param with shape torch.Size([205, 42]) from checkpoint, the shape in current model is torch.Size([205, 85]).

Process finished with exit code 1
